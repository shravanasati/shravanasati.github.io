[{"categories":null,"contents":"I\u0026rsquo;ve been working on EveryNyan, an anonymous social media website exclusive to Adani University students for the past couple of months. This blog outlines some of the implementation details, architecture decisions behind it and the challenges we faced while working on this huge project.\nThe code which is being discussed in this blog can be found here.\nThe Idea Inspired by the movie Social Network, me and my friend Nirav thought of working on a social media website exclusive for our institute\u0026rsquo;s students - with a twist - everything would be anonymous. The core feature of the website would be to create posts on certain boards (eg. confessions, gyan, random\u0026hellip;), where other users could vote them (up or down) and write comments on the post. If it sounds a lot like 4chan and reddit, your observations are astute.\nTech Stack For the website, we decided to use Next.js, since using react alone has been a painful experience for both of us. Next.js tooling is pretty good (except for the initial build times) so we ended up using it. When we started working on the project, the latest Next version was 14, and we still use it, without any plans to upgrade to Next15 in the near future.\nThe integration of Next with shadcn \u0026amp; v0 is also pretty neat.\nFor the database, we chose Firestore on a whim, and regret that decision now (more on that later).\nThe notifications service is written in Go which primarily uses boltdb, and an under-development automatic content-moderation service in Python.\nAnonymity and Exclusivity The biggest challenge in the beginning was authentication - how do I make sure only students of my institute can login on the platform, without revealing their identities. I thought long and hard about it, and after several rounds of discussion with LLMs, I went ahead with a seemingly simple solution - OTPs.\nEvery student of the institute has an email address of this form: name.branchYear@adaniuni.ac.in, I could send OTPs to this email to ensure only a specific set of students can login on the platform.\nOnce the OTP is verified, a random token is generated (a UUID) and stored on cookies as well as the database. The email is only stored in the OTPs collections on the database, and not on the tokens collection.\nThus, we only know who has signed up on the platform, but each login session is independent of the email - there\u0026rsquo;s no way to backtrack a post or a comment author to the email with which the user had logged in.\nThis is a screenshot of a sample document from the OTPs collection (the actual OTP is hashed).\nAnd this is a screenshot of a sample document from the tokens collection.\nThe role field can be either admin or user and is decided during the sign-in process based on the email. An admin user has special privileges - like access to the admin panel (more on that later).\nFor sending these OTP emails, I opted using the Resend API, it has great docs and library support, and with the react-email library, the email content can be a simple react component!\nEverytime a login process is initiated, the email is first validated using a regular expression. If it passes the validation, an OTP is generated, stored on the database with the given email and an email is sent to the user.\nIf the above process was successfull, the user is redirected to a page where they can verify the OTP. If the verification is successfull, they are finally logged in on the platform.\nEach login session initially lasts for 14 days, but upon subsequent requests, the session lifetime keeps increasing.\nLots of CRUD operations What followed the implementation of role-based authentication is boring - lots of CRUD operations - for posts, comments, voting, and reports.\nFlow It was quite simple really - I followed this abstraction strategy for all user interactions with the database.\nStep 1: Write appropriate database functions in lib/firebase using the firebase-admin sdk.\nStep 2: Create server actions for the same in lib/actions, which typically consist of user authentication, and calling the database functions.\nStep 3: Create react components in the components directory, which use the server actions (if applicable).\nStep 4: Use those components in the app router.\nThis was pretty much the flow for all user interactions with the website. Some of them contained extensive client side code as well - especially the feed component which has lazy loading behavior for posts.\nForgot to mention: posts support markdown. We used the MDX library for the markdown editor and and the react-markdown library for rendering the markdown in the post page.\nPost URLs This is not really interesting but I want to mention it anyway - how post slugs are generated. Basically, each post has an ID, which is the only way to identify the post. But once a post is created, its URL might look like this: https://everynyan.tech/post/jus_parody_of_a_song_r3xyny. It is a combination of the first few alphanumeric characters of the title and the post ID. When this endpoint is hit, the server only reads the last 6 characters of the slug - which is the ID of the post. Yeah I stole this idea from reddit. It is quite nice tho - users can see kinda see what the post is about just from the URL.\nComments Rendering comments was quite challenging too. In Firestore, they are stored in a flat structure in the comments collection of each post. Each comment has a parent ID, which is null for top-level comments and the ID of the parent comment for replies. LLMs came to the rescue - I knew I\u0026rsquo;d have to use a tree data structure but I wasn\u0026rsquo;t sure how. I still don\u0026rsquo;t understand how exactly the comments are being rendered along with their replies.\nAdmin Panel The admin panel has four main components - a section to show total logged-in and current active users, a section to resolve reports, a section to broadcast notifications and a section to view security logs.\nIt is only accesible to users with the admin role.\nFeed Initially, there wasn\u0026rsquo;t a \u0026lsquo;feed\u0026rsquo; on EveryNyan, and the only way to browse posts was to go to the specific board page (/board/confessions for example). This wasn\u0026rsquo;t ideal - even I was tired of navigating between different boards to look for new posts.\nSo, we worked on a feed component which is similar to the board feed, except it gathers posts from all the boards. We also added sorting options in the feed - latest, popular, controversial, and engaging, similar to reddit.\nThe feed is shown on the root endpoint (/) to logged-in users.\nWhy a RDBMS would be better I earlier mentioned that we regret using Firestore as the primary database. It\u0026rsquo;s because PostgreSQL is just that much better. Document databases often advertise their flexibility and the lack of schema as their main selling point. I really don\u0026rsquo;t get it. It allows you to not think about the structure of your data properly. Yeah this happened quite some times while working on EveryNyan. Also querying in Firestore is weird. I prefer SQL (Sexy Query Language).\nThe easy setup is nice but there\u0026rsquo;s also ambiguity with the client and admin SDKs. Initially I was using the \u0026ldquo;web\u0026rdquo; SDK which is meant to run from the client-side, but I am interacting with the database only on the server side. This later conflicted with the security rules thing. Maybe I\u0026rsquo;m just stupid but I find it hard to see any advantages in communicating with the database from the client side. I later had to go through the trouble of generating a service account key and switched to the \u0026ldquo;admin\u0026rdquo; SDK. This created another problem - turns out I was using some of the firebase code directly on the client side with client components in Nextjs. When I switched to the admin SDK, some \u0026ldquo;server\u0026rdquo; code was essentially running on client side (which obviously lacks Node standard library) and there were lots of weird import errors. This was a nice learning experience though - I learnt loads about Nextjs.\nThis is not really a negative point since Firebase is quite cheap but I still don\u0026rsquo;t want to pay for database when I can host it for free on a VPS (I have lots of free credits). Cloud functions are available only on the paid plan. And there are also limited read and writes, although we\u0026rsquo;re yet to hit the limit yet.\nSearch is a feature I\u0026rsquo;m excited to implement in EveryNyan. It requires a search engine. There are lots of them in the market - elasticsearch, meilisearch, etc. Search engines need to integrate with the database so they can keep their indexes up-to-date. To do this in Firestore, you need to be on a paid plan. Postgres has native support for Full-Text-Search. If you\u0026rsquo;ve a self-hosted instance, you also needn\u0026rsquo;t worry about hitting the read-write limits.\nThat\u0026rsquo;s why a SQL migration is planned, but I\u0026rsquo;m not sure when I will do it since it\u0026rsquo;s quite a massive undertaking. Not only all the database code needs to be re-written, I need to migrate existing data from Firestore as well.\nNotifications When we first launched EveryNyan publicly, we had great response. We crossed 100 sign-ups within several days even without significant marketing. It was very lively. Confessions, rants and controversial posts here and there. It seemed like the only way forward was upwards.\nUser activity was down by 90% the next week. We had a terribly low user retention. We quickly realized that people are not reopening EveryNyan - rightfully so - people were not yet addicted to it. The only way to re-engage users was notifications. \u0026ldquo;Someone liked your reply üëç\u0026rdquo;, \u0026ldquo;Someone commented on your post üòú\u0026rdquo;, \u0026ldquo;You\u0026rsquo;re getting ratio\u0026rsquo;d üò≤\u0026rdquo; was much needed. Nothing more exciting than fresh notifications disturbing you during productive periods.\nThus began a sweat-inducing, greuling journey to claw EveryNyan back from the brink of irrelevance. Notifications was definitely the biggest and hardest feature to implement so far.\nThis is what I came up with: there would be two kinds of notifications.\nNotifications Center\nThese notifications will be shown behind a bell icon on the navbar and the dock, and will be visible only when the user opens the notifications page. These will be stored in the database and hence will be persistent.\nRealtime notifications\nThese notifications will be delivered instantly to the user via two channels:\nIn-app notifications: These will be shown using toasts. Everytime the website is loaded, a websocket connection with the notifications service will be established. It will then listen for notifications, and show them using toasts whenever the notifications service sends it one.\nPush notifications: Push notifications are delivered to the user\u0026rsquo;s device regardless of whether the website/app (PWA) is open or not. The user first needs to grant the website permission to show notifications. But it is a great way to increase user engagement. Since EveryNyan already had a PWA, I just had to add custom service worker code to react to push notification events.\nThis was my initial architecture design for the notifications service. I chose Go for it because I felt like it.\nSpoiler alert: I didn\u0026rsquo;t add a message queue because\nI thought it would be overengineering for an app of this scale (barely hundred users) I was lazy Notifications Center I started with the notifications center since it was easier to implement.\nFirst things first, posts and comments didn\u0026rsquo;t have an author field before - shit was truly anonymous. But to make notifications possible, we need to know which \u0026ldquo;user\u0026rdquo; (\u0026ldquo;token\u0026rdquo; to be technically accurate) wrote a certain post or comment. So, I added an author field to post and comment types, and when these are created, the session token is added in the \u0026lsquo;author\u0026rsquo; field.\nCurrently, automated notifications are created only on comments. When a comment is written, the post author, and all the parent comment authors in the comment chain (if applicable) are eligible to be notified. Once all the notifiable users are fetched, the notifications collection on firestore is filled with documents with this type.\nexport type NotificationType = { user: string, title: string, description: string, status: \u0026#34;unread\u0026#34; | \u0026#34;read\u0026#34;, link: string, } Next, I created a notifications page which fetches all the documents in the notifications collection where user is the current user and status is unread. Once the page is opened, all unread notifications are marked as read.\nI also added the notification bell icon on the dock and the navbar. The dock icon also shows the unread notification count.\nRealtime Notifications Realtime notifications are handled by a separate service written in Go. The code can be found here.\nIn App Notifications As discussed above, in-app notifications are delivered via websockets. Everytime the website is loaded, a websocket connection is established with the notifications server which is hosted at notifications.everynyan.tech. The server first authenticates the user using firebase. If the authentication is successfull, it adds the websocket connection to a list of active connections.\nWhenever a notification is created, the Next.js server sends a HTTP POST request to this Go server. The request data contains an array of notification object, each having a title, description, link and the user it is targeted to. For each notification object, if the target user is currently online (i.e., the websocket connection exists), a JSON message is sent to it. The client reads the message and shows a toast to the user.\nThis was not very hard to implement but I had some nasty issues.\nThe cookies were not sent from the browser to the notifications service. This was causing authentication failure and all subscription requests were being rejected. After a lot of searching, I came to know that cookies are not sent even to subdomains by default. The fix was to add a Domain attribute while setting cookies with the value everynyan.tech. Now, cookies will be sent to subdomains of everynyan.tech as well.\nThe notifications service sits behind a Nginx reverse proxy. Nginx doesn\u0026rsquo;t accept websocket connections by default. I didn\u0026rsquo;t know much about the nginx configuration so I hacked it together with the help of stackoverflow. I made the following changes:\nlocation / { proxy_pass http://localhost:7924; proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $host; # websocket support proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \u0026#34;upgrade\u0026#34;; } The websocket connections were being dropped after exactly 30s, for no apparent reason. To solve this, I implemented a heartbeat mechanism in which the client and server exchange ping-pong messages every 15s. I didn\u0026rsquo;t debug the exact cause of this issue, I have a hunch this too might be due to some misconfiguration. Push Notifications This was the most painful experience I\u0026rsquo;ve ever had in my entire programming career.\nFor all the features EveryNyan has had yet, I\u0026rsquo;ve had the general idea on how to implement them. Since push notifications was a completely new topic for me, I had no idea how to do it. Lots of reading ensued. MDN had great articles on implementing the client-side of push notifications, and for the server-side I had to make do with the webpush library docs. This is the Go library I used. Its docs were minimal, so I also looked up docs for an equivalent library in Python. It explained the webpush and VAPID flow better. For the next-specific implementation, I borrowed some code from this repo as well.\nSince I kind of understand it now, let me break it down.\nPush notifications are delivered using the webpush protocol. The webpush protocol ensures end-to-end encryption of messages so that only the browser can read it. The authentication is performed using a pair VAPID keys. The public key is shared with the browser and the private key is kept only on the server.\nClient side implementation A service worker must be registered which listens for the push event. A service worker is basically some code which is executed in the background by the browser. It runs in a different thread and has no access to the DOM or other browser APIs like fetch and WebStorage. It can only react to events.\nOnce a service worker is installed and running, it can react to events.\nself.addEventListener(\u0026#39;push\u0026#39;, function(event) { const data = event.data.json(); console.log(\u0026#39;Push received...\u0026#39;, data); self.registration.showNotification(data.title, { body: data.body, icon: data.icon }); }); But before notifications can be shown to users, the user must grant the website permission to show notifications. One caveat with requesting this permission is to do it in response to a user gesture. Some browsers like Firefox outright reject the request if it wasn\u0026rsquo;t done in response to a user gesture, and Chrome has plans to do the same. In EveryNyan, if a user hasn\u0026rsquo;t granted this permission, a toast is shown to them with a \u0026lsquo;subscribe\u0026rsquo; button.\nfunction requestNotificationPermission() { Notification.requestPermission().then(function(permission) { if (permission === \u0026#39;granted\u0026#39;) { subscribeUserToPush(); } }); } Once the permission is granted, the subscribeUserToPush function might look like this:\nfunction subscribeUserToPush() { navigator.serviceWorker.ready.then(function(registration) { const subscribeOptions = { userVisibleOnly: true, applicationServerKey: urlBase64ToUint8Array(publicVapidKey) }; return registration.pushManager.subscribe(subscribeOptions); }) .then(function(pushSubscription) { console.log(\u0026#39;Received PushSubscription:\u0026#39;, JSON.stringify(pushSubscription)); return sendSubscriptionToServer(pushSubscription); }); } The push manager returns a subscription object which looks like this:\n{ \u0026#34;endpoint\u0026#34;: \u0026#34;an endpoint URL...\u0026#34;, \u0026#34;keys\u0026#34;: { \u0026#34;auth\u0026#34;: \u0026#34;auth key\u0026#34;, \u0026#34;p256dh\u0026#34;: \u0026#34;p256dh key\u0026#34;, } } The endpoint is an URL for the push service where our server will send notifications. Each browser vendor maintains their own push service which acts as an intermediary between the browser and server. It checks the VAPID signatures, maintains a message queue if the browser is offline, and finally relays the message to the appropriate service worker. For example, Chrome has FCM, and Mozilla has autopush.\nOnce the subscription is created, it needs to be sent to the notifications server.\nServer side implementation The notifications server receives the push subscription and stores it on a persistent database, which is BoltDB in this case. I used it because a key-value pair is all I needed, the user token and the push subscription.\nThe code for sending push notifications is pretty minimal, the webpush-go library does all the heavy lifting.\nfunc _sendPushNotificationBytes(message []byte, subscription webpush.Subscription) { fmt.Println(\u0026#34;sending this push notification:\u0026#34;, string(message)) resp, err := webpush.SendNotification(message, \u0026amp;subscription, \u0026amp;webpush.Options{ Subscriber: \u0026#34;dev.shravan@proton.me\u0026#34;, VAPIDPublicKey: vapidPublicKey, VAPIDPrivateKey: vapidPrivateKey, Urgency: webpush.UrgencyNormal, }) if err != nil { log.Println(\u0026#34;unable to send push notification\u0026#34;, err) return } defer resp.Body.Close() } Internally, the library encrypts the content using the subscription keys, and prepares a HTTP request to the subscription endpoint (the push service).\nChallenges This might become a bit repetitive but this unexpectedly also did not work on the first try. The issues I encountered this time were:\nInitially, push subscription was being sent directly to the notifications service from the browser. And ofcourse CORS was a pain in the ass. Despite many attempts to fix it, it still wouldn\u0026rsquo;t go away, despite the websocket connection was being successfully established with the service. I don\u0026rsquo;t understand how a GET request works but POST wouldn\u0026rsquo;t even though I specified it in the next config file. ... async headers() { return [ { source: \u0026#34;/(.*)\u0026#34;, headers: [ { key: \u0026#34;Access-Control-Allow-Origin\u0026#34;, value: notificationsServerAddress, }, { key: \u0026#34;Access-Control-Allow-Methods\u0026#34;, value: \u0026#34;GET, POST, OPTIONS\u0026#34; }, { key: \u0026#34;Access-Control-Allow-Headers\u0026#34;, value: \u0026#34;Content-Type, Authorization, Content-Length, X-Requested-With\u0026#34;, }, { key: \u0026#34;Access-Control-Allow-Credentials\u0026#34;, value: \u0026#34;true\u0026#34; }, ], }, ]; }, ... Broadcast The admin panel has a broadcast section as well - from which an admin can broadcast realtime notifications, both in-app and push ones to all the users. Its implementation is also similar, instead of targeting a user, the Go service sends the given notification to all the subscribers. I currently use it to send zomato-style notifications about the hottest post of the day, although I plan to automate it too.\nAutomod Moderating content manually is tiring. The moderation service will be responsible for automatically moderating posts and comments on the platform, to prevent excessive toxicity. The problem is that all the pre-trained models I\u0026rsquo;ve tried so far don\u0026rsquo;t seem to fit our requirements. We want a certain level of toxicity on the platform, otherwise it would be boring. Then there\u0026rsquo;s also an issue with Hindi, none of the models seem to understand Hindi. I tried integrating translation services but the results were not upto the par.\nAfter discussions with multiple ML experts, the conclusion we\u0026rsquo;ve arrived to is to make our own model using our own data. This would be a great learning experience.\nThe code written for this service so far can be found in this repo.\nPlanned Features We\u0026rsquo;ve certainly ticked all the boxes for EveryNyan\u0026rsquo;s MVP. The project is however far from over. There are so many features that are under progress and in the planning stage. The ones I\u0026rsquo;m most excited about is GIF support, images support, search and active users üò≠.\nThanks for reading so far. If you\u0026rsquo;ve any questions or suggestions, I\u0026rsquo;m all ears.\n","date":"December 26, 2024","hero":"/posts/everynyan-architecture/hero.png","permalink":"http://localhost:1313/posts/everynyan-architecture/","summary":"\u003cp\u003eI\u0026rsquo;ve been working on \u003ca href=\"https://everynyan.tech\" target=\"_blank\" rel=\"noopener\"\u003eEveryNyan\u003c/a\u003e, an anonymous social media website exclusive to Adani University students for the past couple of months. This blog outlines some of the implementation details, architecture decisions behind it and the challenges we faced while working on this huge project.\u003c/p\u003e\n\u003cp\u003eThe code which is being discussed in this blog can be found \u003ca href=\"https://github.com/shravanasati/everynyan\" target=\"_blank\" rel=\"noopener\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"the-idea\"\u003eThe Idea\u003c/h3\u003e\n\u003cp\u003eInspired by the movie Social Network, me and my friend \u003ca href=\"https://ni3rav.me\" target=\"_blank\" rel=\"noopener\"\u003eNirav\u003c/a\u003e thought of working on a social media website exclusive for our institute\u0026rsquo;s students - with a twist - everything would be \u003cstrong\u003eanonymous\u003c/strong\u003e. The core feature of the website would be to create posts on certain boards (eg. confessions, gyan, random\u0026hellip;), where other users could vote them (up or down) and write comments on the post. If it sounds a lot like 4chan and reddit, your observations are astute.\u003c/p\u003e","tags":null,"title":"How I built an anonymous and exclusive social media website"},{"categories":null,"contents":"The caesar cipher is one the oldest known ciphers known to mankind. As the story goes, the Roman emperor Julius Caesar used it extensively for military communications. It is a simple substitution cipher and is not fit for any modern usage, since it is a trivial task to break it. However, it may be worthwhile to learn the roots of cryptography, as this cipher leads to more advanced ones like the Vig√®nere cipher and the unbreakable one-time pad cipher. We\u0026rsquo;ll learn how to implement Caesar cipher in Python, and how to break it without knowing the shift involved, using frequency analysis and some statistics. That being said, I am neither a cryptanalyst nor a statistician, so pardon me for any mistakes.\nTheory Let\u0026rsquo;s first understand how the Caesar cipher actually works. Let the text we need to encrypt be hello world. If we assume the shift to be 3, we need to replace each character with a character which is 3 offsets ahead in the alphabets.\nSo, this is show each character will be transformed.\nh (+3) -\u0026gt; k\re (+3) -\u0026gt; h\rl (+3) -\u0026gt; o\rl (+3) -\u0026gt; o\ro (+3) -\u0026gt; r\rw (+3) -\u0026gt; z\ro (+3) -\u0026gt; r\rr (+3) -\u0026gt; u\rl (+3) -\u0026gt; o\rd (+3) -\u0026gt; g It is very simple - all it doing is shifting the characters. If we encounter the end of the alphabets, i.e., z, we circle around and continue from a.\nImage Credits: GeeksForGeeks\nThe formula is essentially: $$ C = (p + shift) mod (26) $$\nwhere,\nC is the ciphered character,\np is the character that needs to be ciphered,\nshift is the amount of shfit.\nImplementation Now let\u0026rsquo;s dive into implementing this encryption scheme in Python.\nimport string class CaesarCipher: def __init__(self, shift: int = 11) -\u0026gt; None: self.shift = shift self.keys_upper = string.ascii_uppercase self.keys_lower = string.ascii_lowercase def __perform_shift(self, text: str, positive: bool) -\u0026gt; str: if positive: shift = self.shift else: shift = -self.shift shifted_text = \u0026#34;\u0026#34; for char in text: if char.isupper(): to_consider = self.keys_upper elif char.islower(): to_consider = self.keys_lower else: shifted_text += char continue index = to_consider.index(char) index += shift index %= 26 shifted_text += to_consider[index] return shifted_text def encrypt(self, text: str) -\u0026gt; str: encrypted_text = self.__perform_shift(text, True) return encrypted_text def decrypt(self, text: str) -\u0026gt; str: decrypted_text = self.__perform_shift(text, False) return decrypted_text That might look a little overwhelming at first, but it\u0026rsquo;s quite easy once you look at it.\nTo summarise, we\u0026rsquo;ve defined a class called CaesarCipher, each instance of whom will work with a specific shift. Since decryption in caesar cipher is virtually the same as encryption (except we just need to shift in reverse), I\u0026rsquo;ve defined a single function which takes two arguments, the text to shift and and a boolean parameter named positive which will decide whether to shift forward or backward.\nOnce that\u0026rsquo;s done, we loop through the given text and first determine whether each character is in upper-case or lower-case. After that, it\u0026rsquo;s just a matter of using the formula mentioned above and adding the shifted character to the shifted_text. Also, I\u0026rsquo;m ignoring any non-alphabetic character encountered in the given text and adding it as it is.\nLet\u0026rsquo;s test it.\nif __name__ == \u0026#34;__main__\u0026#34;: cipher = CaesarCipher() text = \u0026#34;Caesar Cipher isn\u0026#39;t useful at all nowadays. It can be easily broken.\u0026#34; encrypted = cipher.encrypt(text) print(encrypted) decrypted = cipher.decrypt(encrypted) print(decrypted) This is the output of the program.\nNlpdlc Ntaspc tdy\u0026#39;e fdpqfw le lww yzhloljd. Te nly mp pldtwj mczvpy.\rCaesar Cipher isn\u0026#39;t useful at all nowadays. It can be easily broken. Breaking the Cipher Now things get interesting. The idea behind breaking the cipher, without knowing the shift is easy - we know that there are only 26 possible shifts. We can brute-force through all the shifts very quickly and determine which sentence looks like actual English, and not just some random mumblings.\nclass CaesarDecipher: def __init__(self, ciphered_text: str) -\u0026gt; None: self.ciphered_text = ciphered_text def decipher(self): # brute force through all shifts for shift in range(1, 27): print(shift, CaesarCipher(shift).decrypt(self.ciphered_text)) if __name__ == \u0026#34;__main__\u0026#34;: text = \u0026#34;Caesar Cipher isn\u0026#39;t useful at all nowadays. It can be easily broken.\u0026#34; enc = CaesarCipher(21).encrypt(text) CaesarDecipher(enc).decipher() This works, but looking through all those shifted texts is still a pain. What if we could come up with a method to show, maybe the 5 most senseful transformations?\nEnter frequency analysis.\nThe English language is biased towards certain letters. Those letters appear more often than their peers. Prime examples of such letters are e, t, a, o and so on. This cipher, by shifting the letters, also shifts their usage proportions.\nThis bar chart shows the percentage appearance of each letter in English texts. When the caesar cipher is applied, this chart is also translated sideways by the amount of shift.\nThus, we need to find the shift in the chart (i.e., usage proportions), and by reversing the shift we\u0026rsquo;ll arrive at the original text.\nLet\u0026rsquo;s code that now.\nimport heapq import string from ciphers import CaesarCipher import scipy.stats class CaesarDecipher: def __init__(self, ciphered_text: str) -\u0026gt; None: self.ciphered_text = ciphered_text # taken from https://norvig.com/mayzner.html self.freq_table = { \u0026#34;e\u0026#34;: 12.49, \u0026#34;t\u0026#34;: 9.28, \u0026#34;a\u0026#34;: 8.04, \u0026#34;o\u0026#34;: 7.64, \u0026#34;i\u0026#34;: 7.57, \u0026#34;n\u0026#34;: 7.23, \u0026#34;s\u0026#34;: 6.51, \u0026#34;r\u0026#34;: 6.28, \u0026#34;h\u0026#34;: 5.05, \u0026#34;l\u0026#34;: 4.07, \u0026#34;d\u0026#34;: 3.82, \u0026#34;c\u0026#34;: 3.34, \u0026#34;u\u0026#34;: 2.73, \u0026#34;m\u0026#34;: 2.51, \u0026#34;f\u0026#34;: 2.40, \u0026#34;p\u0026#34;: 2.14, \u0026#34;g\u0026#34;: 1.87, \u0026#34;w\u0026#34;: 1.68, \u0026#34;y\u0026#34;: 1.66, \u0026#34;b\u0026#34;: 1.48, \u0026#34;v\u0026#34;: 1.05, \u0026#34;k\u0026#34;: 0.54, \u0026#34;x\u0026#34;: 0.23, \u0026#34;j\u0026#34;: 0.16, \u0026#34;q\u0026#34;: 0.12, \u0026#34;z\u0026#34;: 0.09, } @staticmethod def get_frequency_distribution(s: str): table_count: dict[str, int] = {c: 0 for c in string.ascii_lowercase} for char in s: if char.isalpha(): table_count[char.lower()] += 1 total = sum(table_count.values()) table_freq = {k: v / total * 100 for k, v in table_count.items()} return table_freq def decipher(self, threshold: int): shifted_ciphers: list[str] = [] # brute force through all shifts for shift in range(1, 27): shifted_ciphers.append(CaesarCipher(shift).decrypt(self.ciphered_text)) # perform frequency analysis using chi square method chi_table: dict[str, float] = {} expected_dist = list(self.freq_table.values()) expected_sum = sum(expected_dist) for sentence in shifted_ciphers: observed_dist = list(self.get_frequency_distribution(sentence).values()) observed_sum = sum(observed_dist) # normalize observed distribution observed_dist = [x * (expected_sum / observed_sum) for x in observed_dist] chi_square, p_value = scipy.stats.chisquare( f_obs=observed_dist, f_exp=expected_dist ) chi_table[sentence] = chi_square * (1 - p_value) return [ i[0] for i in heapq.nsmallest( min(max(threshold, 1), 26), chi_table.items(), key=lambda item: item[1] ) ] if __name__ == \u0026#34;__main__\u0026#34;: text = \u0026#34;\u0026#34;\u0026#34; This bar chart shows the percentage appearance of each letter in English texts. When the caesar cipher is applied, this chart is also translated sideways by the amount of shift. Thus, we need to find the shift in the chart (i.e., usage proportions), and by reversing the shift we\u0026#39;ll arrive at the original text. \u0026#34;\u0026#34;\u0026#34;.strip() ciphered = CaesarCipher(13).encrypt(text) print(ciphered) threshold = 3 print(\u0026#34;Top {} picks:\u0026#34;.format(threshold)) for i, pick in enumerate(CaesarDecipher(ciphered).decipher(threshold)): print(i + 1, pick) print(\u0026#34;\\n--------\\n\u0026#34;) Let\u0026rsquo;s break it piece-by-piece.\ndef __init__(self, ciphered_text: str) -\u0026gt; None: self.ciphered_text = ciphered_text # taken from https://norvig.com/mayzner.html self.freq_table = { \u0026#34;e\u0026#34;: 12.49, \u0026#34;t\u0026#34;: 9.28, \u0026#34;a\u0026#34;: 8.04, \u0026#34;o\u0026#34;: 7.64, \u0026#34;i\u0026#34;: 7.57, \u0026#34;n\u0026#34;: 7.23, ... This is the frequency table for the English language, which would be our expected distribution. These values add up to almost 100%.\n@staticmethod def get_frequency_distribution(s: str): table_count: dict[str, int] = {c: 0 for c in string.ascii_lowercase} for char in s: if char.isalpha(): table_count[char.lower()] += 1 total = sum(table_count.values()) table_freq = {k: v / total * 100 for k, v in table_count.items()} return table_freq This is a helper function which generates a frequency distribtion table from the given text. We\u0026rsquo;re just counting the number of each character, and then converting them into percentages. Note how we\u0026rsquo;ve converted each alphabetic character to lower case, to compare them against our expected frequency distribution.\ndef decipher(self, threshold: int): shifted_ciphers: list[str] = [] # brute force through all shifts for shift in range(1, 27): shifted_ciphers.append(CaesarCipher(shift).decrypt(self.ciphered_text)) # perform frequency analysis using chi square method chi_table: dict[str, float] = {} expected_dist = list(self.freq_table.values()) expected_sum = sum(expected_dist) ... We\u0026rsquo;re starting out with the same brute-force logic, but accumulating those transformed sentences instead of printing them.\nNow, for frequency analysis, we would be comparing each shifted sentence\u0026rsquo;s frequency distribution to the expected frequency distribution. To compare how close those distributions are, we would be employing the chi-squared test.\nWithout going into much detail of what it is, it returns two numbers, the chi-squared statistic and the p-value. The lower the first number is, the better fit is the observed distribution with the expected distribution. The higher the second number is, the less indifference is between the two distributions. We will generate a combined score from both those numbers as such:\n$$ score = \\chi^2 * (1 - p) $$\nAfter we\u0026rsquo;ve calculated the score for all the transformations, we\u0026rsquo;ll sort them according to their score, and return, for example, the 5 transformations with the smallest scores. That\u0026rsquo;s what the rest of the code does.\n... for sentence in shifted_ciphers: observed_dist = list(self.get_frequency_distribution(sentence).values()) observed_sum = sum(observed_dist) # normalize observed distribution observed_dist = [x * (expected_sum / observed_sum) for x in observed_dist] chi_square, p_value = scipy.stats.chisquare( f_obs=observed_dist, f_exp=expected_dist ) chi_table[sentence] = chi_square * (1 - p_value) return [ i[0] for i in heapq.nsmallest( min(max(threshold, 1), 26), chi_table.items(), key=lambda item: item[1] ) ] The decipher method takes a threshold parameter, which indicates how many sentences we need to return. According to my tests, larger sentences, like in the example below are usually the first pick. Shorter sentences often fail because of the lack of enough characters. To combat that, increase the threshold parameter.\nif __name__ == \u0026#34;__main__\u0026#34;: text = \u0026#34;\u0026#34;\u0026#34; This bar chart shows the percentage appearance of each letter in English texts. When the caesar cipher is applied, this chart is also translated sideways by the amount of shift. Thus, we need to find the shift in the chart (i.e., usage proportions), and by reversing the shift we\u0026#39;ll arrive at the original text. \u0026#34;\u0026#34;\u0026#34;.strip() ciphered = CaesarCipher(13).encrypt(text) print(ciphered, \u0026#34;\\n\u0026#34;) threshold = 3 print(\u0026#34;Top {} picks:\u0026#34;.format(threshold)) for i, pick in enumerate(CaesarDecipher(ciphered).decipher(threshold)): print(i + 1, pick) print(\u0026#34;\\n--------\\n\u0026#34;) Here\u0026rsquo;s the output:\nGuvf one puneg fubjf gur crepragntr nccrnenapr bs rnpu yrggre va Ratyvfu grkgf. Jura gur pnrfne pvcure vf nccyvrq, guvf puneg vf nyfb genafyngrq fvqrjnlf ol gur nzbhag bs fuvsg. Guhf, jr arrq gb svaq gur fuvsg va gur puneg (v.r., hfntr cebcbegvbaf), naq ol erirefvat gur fuvsg jr‚Äôyy neevir ng gur bevtvany grkg.\rTop 3 picks:\r1 This bar chart shows the percentage appearance of each letter in English texts. When the caesar cipher is applied, this chart is also translated sideways by the amount of shift. Thus, we need to find the shift in the chart (i.e., usage proportions), and by reversing the shift we‚Äôll arrive at the original text.\r--------\r2 Uijt cbs dibsu tipxt uif qfsdfoubhf bqqfbsbodf pg fbdi mfuufs jo Fohmjti ufyut. Xifo uif dbftbs djqifs jt bqqmjfe, uijt dibsu jt bmtp usbotmbufe tjefxbzt cz uif bnpvou pg tijgu. Uivt, xf offe up gjoe uif tijgu jo uif dibsu (j.f., vtbhf qspqpsujpot), boe cz sfwfstjoh uif tijgu xf‚Äômm bssjwf bu uif psjhjobm ufyu.\r--------\r3 Guvf one puneg fubjf gur crepragntr nccrnenapr bs rnpu yrggre va Ratyvfu grkgf. Jura gur pnrfne pvcure vf nccyvrq, guvf puneg vf nyfb genafyngrq fvqrjnlf ol gur nzbhag bs fuvsg. Guhf, jr arrq gb svaq gur fuvsg va gur puneg (v.r., hfntr cebcbegvbaf), naq ol erirefvat gur fuvsg jr‚Äôyy neevir ng gur bevtvany grkg.\r-------- While I was working on this, I discovered a website called boxentriq. It works even for small sentences. They\u0026rsquo;ve not shared their implementation, but their scores are much more better for actual sentences compared to incorrect ones. I assume some NLP is at play.\nAnyway, that\u0026rsquo;s all for this blog. If you want to explore some more ciphers, take a look at the Vigenere cipher, one-time pad cipher \u0026amp; the enigma machine.\n","date":"August 17, 2024","hero":"/posts/caesar-cipher/hero.jpg","permalink":"http://localhost:1313/posts/caesar-cipher/","summary":"\u003cp\u003eThe caesar cipher is one the oldest known ciphers known to mankind. As the story goes, the Roman emperor Julius Caesar used it extensively for military communications. It is a simple \u003ca href=\"https://en.wikipedia.org/wiki/Substitution_cipher\" target=\"_blank\" rel=\"noopener\"\u003esubstitution cipher\u003c/a\u003e and is not fit for any modern usage, since it is a trivial task to break it. However, it may be worthwhile to learn the roots of cryptography, as this cipher leads to more advanced ones like the Vig√®nere cipher and the unbreakable one-time pad cipher. We\u0026rsquo;ll learn how to implement Caesar cipher in Python, and how to break it without knowing the shift involved, using frequency analysis and some statistics. That being said, I am neither a cryptanalyst nor a statistician, so pardon me for any mistakes.\u003c/p\u003e","tags":null,"title":"Understanding and breaking the Caesar cipher"},{"categories":null,"contents":"I often tend to take a look at the websites I\u0026rsquo;ve deployed - to check whether they are still online or not (I should use monitoring services). It also helps with the gloom that appears on my face when I view the website analytics.\nOne such time was the fine summer morning of 13th June 2024. I woke up late and checked my mobile for notifications (there weren\u0026rsquo;t any). I decided to perform the health check of my websites. All of them were working fine, except animeviz. When I opened it, it showed the default 502 Bad Gateway error page of nginx - which meant the python server was down. I was flabbergasted really - there weren\u0026rsquo;t any recent code changes (last commit on the repository was almost 20 days ago). I kind of saw what was coming for me - a long debugging session. I slept back again and didn\u0026rsquo;t start solving the problem until 11:30 AM.\nBackground For those who don\u0026rsquo;t know, animeviz is a website that draws visualizations on your anime lists. It uses the MyAnimeList API to fetch user animelist, and then processes the data and draws some visualizations on it. The backend server is written in Python using the Flask library. It is deployed on a small DigitalOcean Droplet (1GB RAM, 25GB disk).\nInvestigation I refreshed the page a couple more times and the result was unexpectedly the same üòî. This meant the problem was something huge. I\u0026rsquo;d rather not admit this, but animeviz has issues with memory leaks. Overtime, its memory usage just keeps increasing, reaching 100% when the Linux OOM (out of memory) killer kills the gunicorn process which is serving the site. This used to be a weekly occurence previously, and I had to manually SSH into the VPS and restart the server. Of course this wasn\u0026rsquo;t ideal, I tried minimizing the memory leaks and modified the systemd service to restart the gunicorn process everytime it is shut down. This solved the downtime problem to a large extent and animeviz didn\u0026rsquo;t go down in the last two months (until 13th June) since I made those changes.\nThe fact that animeviz was still down meant that the issue wasn\u0026rsquo;t with memory leaks, since systemd would have already restarted it in that case. I quickly went over to the DigitalOcean dashboard and looked at the resource insights. The CPU has been maxing out since the last 5 hours (from about 6:15am) and the memory graph was a zig zag curve wobbling between 50% and 70%. Yikes!\nI quickly SSH\u0026rsquo;d into the droplet and looked at the animeviz service logs using journalctl.\nsudo journalctl -u animeviz This opened the service logs from the first line and I pressed G to navigate to the last line (vim keybindings work on the less pager). This operation took about 30s, and when I reached the end of the file, the line count showed a number around 9lakhs! The navigation in the file itself was proving very difficult and slow. I googled how to remove old logs in journalctl and found a command.\nsudo journalctl -u animeviz --vacuum-time 1d This command removed all logs older than a day. Navigation was still slow (around 4 lakh lines remained). I still looked through the logs, the systemd restart counter was at 200 (üòß). The same error was popping thorughout the logs, something related to mysql database connection. I was kind of relieved, since it was safe to assume that there wasn\u0026rsquo;t anything wrong with my code. This was part of the traceback:\nJun 13 10:50:30 animeviz gunicorn[879657]: sqlalchemy.exc.DatabaseError: (mysql.connector.errors.DatabaseError) 2003 (HY000): Can\u0026#39;t connect to MySQL server on \u0026#39;localhost:3306\u0026#39; (111) I checked the status of the mysql service.\nsudo systemctl status mysql The status part showed \u0026ldquo;Server upgrade in process\u0026hellip;\u0026rdquo;. Huh, that was confusing. I restarted both mysql and animeviz services. The situation didn\u0026rsquo;t change. I powered off the droplet and powered it on again, to no avail. One of the most powerful tools under my belt had just failed, and about 20 mins have been lost.\nI tried connecting with the database using the mysql client, as root.\nsudo mysql -u root \u0026gt; Can\u0026#39;t connect to MySQL server on \u0026#39;localhost:3306\u0026#39; (111) So, I cannot connect with the official client either. That means the issue is definitely with the mysql server/daemon.\nThe Culprit I checked the mysql logs using journalctl.\nsudo journalctl -u mysql This is what those logs looked like:\nmysql.service: Scheduled restart job, restart counter is at 180.\rJun 13 11:26:36 animeviz systemd[1]: Stopped MySQL Community Server.\rJun 13 11:26:36 animeviz systemd[1]: mysql.service: Consumed 39.062s CPU time.\rJun 13 11:26:36 animeviz systemd[1]: Starting MySQL Community Server...\rJun 13 11:28:38 animeviz systemd[1]: mysql.service: Main process exited, code=exited, status=1/FAILURE\rJun 13 11:28:38 animeviz systemd[1]: mysql.service: Failed with result \u0026#39;exit-code\u0026#39;.\rJun 13 11:28:38 animeviz systemd[1]: Failed to start MySQL Community Server.\rJun 13 11:28:38 animeviz systemd[1]: mysql.service: Consumed 39.131s CPU time. Notice the restart counter. I was quite annoyed to see that the actual error wasn\u0026rsquo;t reported in these logs. I checked the service status again, and this time it had a different message similar to: \u0026ldquo;Server upgrade failed, timeout due to lock\u0026rdquo;.\nI stopped the animeviz service since restarting them was just consuming more resources, and I had a hunch that the animeviz server, repeatedly trying to establish a connection with mysql might be interfering iwth the server upgrade.\nI googled \u0026ldquo;server upgrade in progress mysql\u0026rdquo; and the first two links were forums titled \u0026ldquo;MySQL stuck in an upgrade loop with high CPU usage\u0026rdquo;. Jackpot üòé.\nFor the reader\u0026rsquo;s reference: this is the first and second forum I found.\nIn that first forum, I found out that mysql logs the actual errors in this file: /var/log/mysql/error.log. I used the tail command given in the forum.\ntail /var/log/mysql/error.log Since those logs were in the /var directory, I can\u0026rsquo;t read them as of the writing date, but I vaguely remember it was along the lines of \u0026ldquo;timeout due to locking\u0026rdquo;. The error was not similar to the two forum posts.\nRegardless, I thought I should pursue the steps given in the second forum (stack exchange always works üôá). I followed these steps in order:\nStopped the mysql service. sudo systemctl stop mysql Ran the server with minimal upgrade flag. /usr/sbin/mysqld --upgrade minimal As pointed out in the answer by KK Chauhan, the command failed, giving an error like unable to create sock file. They also answered that this could be solved by creating the following directory.\nsudo mkdir /var/run/mysqld This wasn\u0026rsquo;t enough (the server had the same error as above), I also had to change the ownership of this folder.\nsudo chown mysql:mysql /var/run/mysqld I ran the mysql server again with the minimal upgrade flag. The server started this time! WOOHOO! I tried connecting with the mysql client. That worked too. Now I felt like I was getting somewhere. I stopped the server since this wasn\u0026rsquo;t going to cut it: it was a hacky solution and in no time it would come to bite me again.\nI created database backup. for DB in $(mysql -u root -pPassword -e \u0026#39;show databases\u0026#39; -s --skip-column-names); do mysqldump -u root -pPassword $DB \u0026gt; \u0026#34;$DB.sql\u0026#34;; done Backing up the database was necessary, since a couple of tables had rows in 4-5 digits.\nI started the mysql service in hopes it would be back running. sudo systemctl start mysql My hopes were shattered. JournalCTL had superficial error logs. Again, as pointed in the stack exchange answer, I checked out the mysql error log file. And it showed the error same as that in the first forum! Talk about non-linear storytelling.\nI went back to the first forum and read the solution (the OP answered it themselves). Basically, the error was about mysql failing to upgrade due to limited thread_stack size. I have no idea what that parameter is about. Regardless, I navigated to /etc/mysql directory to double the thread_stack. On ls -R, I noticed there were a lot of configuration files. I opened the ./mysql.conf.d/mysqld.cnf file in vim and changed the thread_stack parameter from 128Kto 256K.\nAs quickly as my hopes went up, they came crashing down as the mysql server didn\u0026rsquo;t start and threw the same error at my face, again. I had a hunch that I\u0026rsquo;d have to edit all the files with the thread_stack parameter.\nSo that\u0026rsquo;s what I did. I searched all the files with the text thread_stack in the /etc/mysql directory.\nsudo grep thread_stack -R . It showed three files - one of which I had already edited. I did the same change for the other two files. It had to work this time. If it didn\u0026rsquo;t, I\u0026rsquo;d have to purge mysql and reinstall (and re-setup), which is quite a lengthy process.\nThe moment of truth: IT FINALLY WORKED! And this time, fr, the server started.\nI started the animeviz service too. Visited animeviz.ninja. It was live. After an hour.\nOne thing, which still remains a mystery is what caused the mysql server upgrade. Was it scheduled, or triggered by someone (üò®), or mysql suddenly decided it wants to upgrade? I\u0026rsquo;d appreciate if some subject expert helped me resolve this question.\nBye That\u0026rsquo;s all folks. I hope you guys enjoyed and learnt something from this blog. I think this would come in handy next time something like this happens, to me, or to anyone else.\n(here\u0026rsquo;s my referral link for digitalocean, sign up using it, you get free credits, I get free credits)\n","date":"June 20, 2024","hero":"/posts/production-berserk/hero.jpg","permalink":"http://localhost:1313/posts/production-berserk/","summary":"\u003cp\u003eI often tend to take a look at the websites I\u0026rsquo;ve deployed - to check whether they are still online or not (I should use monitoring services). It also helps with the gloom that appears on my face when I view the website analytics.\u003c/p\u003e\n\u003cp\u003eOne such time was the fine summer morning of 13th June 2024. I woke up late and checked my mobile for notifications (there weren\u0026rsquo;t any). I decided to perform the health check of my websites. All of them were working fine, except \u003ca href=\"https://animeviz.ninja\" target=\"_blank\" rel=\"noopener\"\u003eanimeviz\u003c/a\u003e. When I opened it, it showed the default 502 Bad Gateway error page of nginx - which meant the python server was down. I was flabbergasted really - there weren\u0026rsquo;t any recent code changes (last commit on the repository was almost 20 days ago). I kind of saw what was coming for me - a long debugging session. I slept back again and didn\u0026rsquo;t start solving the problem until 11:30 AM.\u003c/p\u003e","tags":null,"title":"How Everything Suddenly Goes Berserk in Production"},{"categories":null,"contents":"So I have this one friend who constantly boasts about his sub-200 WPM typing speed. I tried competing with him but couldn\u0026rsquo;t manage it above 100 WPM. I decided it was not worth the effort since I could beat him with the help of Python! :)\nWe\u0026rsquo;ll be using the awesome selenium library for this task.\nPrerequisites Just intermediate-level knowledge of Python and HTML tags. That\u0026rsquo;s all. I\u0026rsquo;ll be explaining selenium along the way. This is essentially a selenium tutorial since I love project-based learning.\nWhat is Selenium? Selenium is a browser automation library which has bindings available for several programming languages, including Python. We\u0026rsquo;ll be exploiting the power of the selenium webdriver, which allows us to control the browser programmatically.\nSelenium is mostly used for automated testing purposes, and occasionally to do such cool things.\nInternally, selenium uses webdrivers provided by browsers such as the GeckoDriver for Firefox, the Chromedriver for Chrome and so on to control the browser.\nDevelopment Setup We need three things installed on our system to write selenium scripts - the language libraries, the browser we want to use, and the driver for that specific browser.\nThis used to be a cumbersome process for developers and often led to a lot of confusion among beginners, especially around having drivers on PATH.\nFortunately, the selenium team developed the selenium manager which automatically manages the browser and driver, and ships with language libraries.\nThus all we need to do is to install the selenium library for Python.\nThis is simple:\npip install selenium==4.20.0 (v4.20.0 because I\u0026rsquo;m also using it.)\nIdeally, you should also create a virtual environment to isolate your project dependencies from the system-wide ones. Read more about it here. Some Gotchas Before getting into writing code, I\u0026rsquo;d like to discuss some of the selenium gotchas.\nThe browser knows it is being remotely controlled.\nNone of your logins (cookies to be precise) of any websites, browser settings and extensions will be remembered when the browser is being controlled by selenium.\nEven the browsing history and cookies obtained during the session will not be saved.\nThat means, if you‚Äôre performing an action like logging in or clicking the cookie popup, you‚Äôd have to do it every time a new session is opened.\nDon‚Äôt poke around too much with big websites, they have mechanisms to know a bot is playing with their website. They might IP block you.\nSelenium can‚Äôt get around smart captchas like reCAPTCHA, or any captchas for that matter, on its own.\nWriting the Script Let\u0026rsquo;s get to writing the code now.\nFirst of all, let\u0026rsquo;s import the selenium library.\nfrom selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.wait import WebDriverWait import selenium.webdriver.support.expected_conditions as EC from selenium.common.exceptions import NoSuchElementException You might notice that\u0026rsquo;s a lot of imports, but bear with me - I\u0026rsquo;ll explain them as we use them.\nLet\u0026rsquo;s start with opening the monkeytype website using selenium.\ndriver = webdriver.Firefox() driver.get(\u0026#34;https://monkeytype.com\u0026#34;) If you want to use chrome, or any other browser use the respective class present in the webdriver module. For example, Chrome, Edge, Safari, Ie (why?) and so on. Also, if the browser immediately exits after loading the website, add an input() statement at the end of the file.\nThe first time you\u0026rsquo;ll run this script, it might take a lot of time before the browser window is opened since selenium downloads the necessary webdrivers first. After running the script, this is what we see.\nSo we\u0026rsquo;ve visited the MonkeyType website using Selenium. The next step is to interact with the website.\nThe first thing we see is the cookie popup, and since I mentioned before that browsers remove all cookies obtained during the selenium, we\u0026rsquo;ll have to click either accept or reject button everytime the website is opened.\nLet\u0026rsquo;s code clicking the \u0026ldquo;reject non-essential\u0026rdquo; button.\nEverytime we want to interact with a web element using selenium, like clicking or simulate typing, we have to find that element. Selenium provides a neat way to do it - we can use CSS selectors, IDs, classes, XPATH, tag names and so on.\nOpen the browser developer tools (CTRL+Shift+I or F12) and go to the Inspector tab. Click on the element picker icon (or press Ctrl+Shift+C) and click on the reject non-essential button.\nYou\u0026rsquo;ll see the resulting HTML code responsible for that button. Upon looking at the HTML definition of that button, you\u0026rsquo;ll see that this button has a class of rejectAll.\nprint(\u0026#34;clicking reject all\u0026#34;) rejectbutton = driver.find_element(By.CSS_SELECTOR, \u0026#34;button.rejectAll\u0026#34;) rejectbutton.click() The find_element method of the driver takes two arguments - the locator strategy and the locator value. We have used CSS selectors here. The By class (more like an enum) has various locator strategies defined as constants.\nThe click method is straight-forward, we store the element we found in a variable and then call the click method on it.\nNote that the find_element method will raise the NoSuchElementException if it doesn\u0026rsquo;t find the element based on the locator we provided.\nNext, we need to configure the test options. I\u0026rsquo;ve decided to go for the words test with 25 words. That\u0026rsquo;s also easy - just find the elements and click them.\nOpen the dev tools again and pick the words option. We don\u0026rsquo;t find any ID or class associated with this element. Let the dev tools do the heavy lifting, just right click on the element and copy its selector. Do the same for selecting the number of words.\nprint(\u0026#34;selecting words -\u0026gt; 25\u0026#34;) words_btn = driver.find_element(By.CSS_SELECTOR, \u0026#34;div.mode \u0026gt; div:nth-child(2)\u0026#34;) words_btn.click() wait = WebDriverWait(driver, timeout=10, poll_frequency=.5, ignored_exceptions=errors) wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \u0026#34;.wordCount \u0026gt; div:nth-child(2)\u0026#34;))) words_btn = driver.find_element(By.CSS_SELECTOR, \u0026#34;.wordCount \u0026gt; div:nth-child(2)\u0026#34;) words_btn.click() The finding and clicking of elements is the same - however we\u0026rsquo;re also using explicit waits here.\nWaits in selenium, as the name implies, used to wait. While working with selenium, we often need to wait after we\u0026rsquo;ve performed a certain action. Perhaps an element we interacted changes a lot of the webpage structure, or redirects us somewhere else. In these cases, we use waits.\nThere are two kinds of waits in selenium - implicit and explicit waits. Implicit waits are simple, once set for the driver, everytime selenium locates an element it waits for the given number of seconds while simultaneously trying to locate the element. If selenium is unable to find the element even after waiting, it will throw the NoSuchElementException.\ndriver.implicitly_wait(2) This will set an implicit wait for the driver. Note that this is a global setting that applies to every element location call for the entire session. If the element is found, for example, in a second, driver will not wait any longer. It will return element reference and continue executing further.\nAs you might\u0026rsquo;ve already guessed, using implicit waits is not a very good idea. You can never know how long will it take for requests to finish. A better solution is to use explicit waits.\nExplicit waits have a condition associated with them that must evaluate to True before the code can continue executing. An explicit wait can be initialized as follows:\nwait = WebDriverWait(driver, timeout=10, poll_frequency=.5, ignored_exceptions=errors) The driver and timeout argument must be given. The timeout is the max duration you want driver to spend waiting until the given condition evaluates to True. Obviously, you don\u0026rsquo;t want your code to spend an eternity waiting in case the condition can never be True. A TimeoutException will be thrown when the timeout is over.\nThe rest two arguments, poll_frequency and ignored_exeptions tell selenium how long to sleep before evaluating the condition and which exceptions to ignore, if any occur, respectively.\nThe wait object has two public methods - until and until_not.\nwait.until(lambda d: element.is_displayed()) Both until and until_not methods accept a function which takes the driver as argument, and returns a truthy value which will then stop the wait.\nWe can use these methods to wait until a element is visible, present in the DOM and so on. The expected_conditions module we imported above comes with a lot of helpful pre-defined functions that do these tasks. Make sure to check their documentation.\nWe\u0026rsquo;ll use the presence_of_element_located function which returns a predicate that will make the webdriver wait until that element is present in DOM. It takes a locator argument, which is just a tuple of the locator strategy and the locator value.\nwait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \u0026#34;.wordCount \u0026gt; div:nth-child(2)\u0026#34;))) Waiting until the elements are present, and in a desired state before interacting with them is always a good idea.\nDo NOT ever use implicit and explicit waits together in a script. They lead to unpredictable behavior. Whoops, that was a lot of theory. Let\u0026rsquo;s take a look at code accumulated so far. It should open the monkeytype website, click on the \u0026ldquo;reject non-essential\u0026rdquo; button, select words mode and 25 words.\nfrom selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.wait import WebDriverWait import selenium.webdriver.support.expected_conditions as EC from selenium.common.exceptions import NoSuchElementException driver = webdriver.Firefox() driver.get(\u0026#34;https://monkeytype.com\u0026#34;) print(\u0026#34;clicking reject all\u0026#34;) rejectbutton = driver.find_element(By.CSS_SELECTOR, \u0026#34;button.rejectAll\u0026#34;) rejectbutton.click() print(\u0026#34;selecting words -\u0026gt; 25\u0026#34;) words_btn = driver.find_element(By.CSS_SELECTOR, \u0026#34;div.mode \u0026gt; div:nth-child(2)\u0026#34;) words_btn.click() wait = WebDriverWait(driver, timeout=10, poll_frequency=.5, ignored_exceptions=errors) wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \u0026#34;.wordCount \u0026gt; div:nth-child(2)\u0026#34;))) words_btn = driver.find_element(By.CSS_SELECTOR, \u0026#34;.wordCount \u0026gt; div:nth-child(2)\u0026#34;) words_btn.click() Finally, we just need to simulate typing into the monkeytype website.\nfrom time import sleep sleep(2) print(\u0026#34;writing into input\u0026#34;) prompt = driver.find_element(By.CSS_SELECTOR, \u0026#34;input#wordsInput\u0026#34;) words = driver.find_elements(By.CSS_SELECTOR, \u0026#34;div.word\u0026#34;) for word in words: text = \u0026#34;\u0026#34;.join([w.text for w in word.find_elements(By.TAG_NAME, \u0026#34;letter\u0026#34;)]) text = text.strip() prompt.send_keys(f\u0026#34;{text} \u0026#34;) input(\u0026#34;press enter to exit\u0026#34;) driver.quit() First of all, I\u0026rsquo;m doing a dirty practice here by sleeping two seconds instead of using explicit waits. Yeah I will not justify using it.\nThe code finds the input element and stores it in the prompt variable. To simulate typing into the input, we can use the send_keys('text') method on prompt.\nNext, we need to find the words. Similar to the find_element function, find_elements function returns a list of all the elements it could find by the locator. This is because each word on monkeytype is defined like this:\n\u0026lt;div class=\u0026#34;word active\u0026#34;\u0026gt; \u0026lt;letter\u0026gt;l\u0026lt;/letter\u0026gt; \u0026lt;letter\u0026gt;i\u0026lt;/letter\u0026gt; \u0026lt;letter\u0026gt;k\u0026lt;/letter\u0026gt; \u0026lt;letter\u0026gt;e\u0026lt;/letter\u0026gt; \u0026lt;/div\u0026gt; Once we\u0026rsquo;ve grabbed all the words, we need to construct the word by joining the text inside all letter tags. We do it by looping over the word container, then inside each word, we call find_elements for letter tags and grab its inner text. After combining all the letters, we remove all the whitespaces just in case. Finally, we call the send_keys function to type the constructed word and a space (!important).\nThe driver.quit() method closes the browser window. We\u0026rsquo;re waiting for user input before quitting so you can admire the typing speed you can never achieve.\nThe Result I recorded my screen and sent it to my friend. Let\u0026rsquo;s just say, his reaction was pleasing to witness.\nHere\u0026rsquo;s the full code:\nWebsites tend to change a lot - that means the HTML structure changes too. So in the near future, this script might no longer work due to element locators becoming obsolete. Between the time I first wrote the script and the publishing date of this blog, I\u0026rsquo;ve had to modify it twice. But with the knowledge and procedure I\u0026rsquo;ve shared above, you will be able to fix it.\nAnother thing - I tested this script on Firefox, Chrome and Edge - only Firefox had Infinite WPM, the chromium based browsers only did around 400wpm. This just proves why Firefox is superior /s.\nGoing forward The script is barely crossing 30 lines, yet we\u0026rsquo;ve managed to automate monkeytype with selenium.\nYou can modify this program to add some interactivity - like choosing a test mode and further adding options to change parameters like words and time. The code is also quite unorganized. I leave this as exercise to you, the reader; and if you manage to do it, please let me know on my socials (listed below).\nUntil then, bye.\n","date":"May 16, 2024","hero":"/posts/selenium/hero.png","permalink":"http://localhost:1313/posts/selenium/","summary":"\u003cp\u003eSo I have this one friend who constantly boasts about his sub-200 WPM typing speed. I tried competing with him but couldn\u0026rsquo;t manage it above 100 WPM. I decided it was not worth the effort since I could beat him with the help of Python! :)\u003c/p\u003e\n\u003cp\u003eWe\u0026rsquo;ll be using the awesome \u003ca href=\"https://www.selenium.dev/\" target=\"_blank\" rel=\"noopener\"\u003eselenium\u003c/a\u003e library for this task.\u003c/p\u003e\n\u003cdiv style=\"margin-top: 3rem;\"\u003e\u003c/div\u003e\n\u003ch2 id=\"prerequisites\"\u003ePrerequisites\u003c/h2\u003e\n\u003cp\u003eJust intermediate-level knowledge of Python and HTML tags. That\u0026rsquo;s all. I\u0026rsquo;ll be explaining selenium along the way. This is essentially a selenium tutorial since I love project-based learning.\u003c/p\u003e","tags":null,"title":"Achieving Infinite WPM in MonkeyType using Python"}]