---
title: "Breaking down Reverie's Chat Pipeline"
publishedAt: "2025-11-09"
summary: In this blog, I break down Reverie's chat with journals feature (yes, it uses RAG).
hero: hero.jpg
draft: true
---

For the past few months, I've been working on [reverie](https://reverieai.vercel.app), an AI-powered journaling app that provides actionable insights and tracks mental well-being. One of the core features of the app is the "chat with journals" feature, in which users can ask reverie about their journals, and it would answer in accordance with their journals, and also list the sources it's generating its answer from.

![chat example](/reverie-chat-pipeline/example.png)

Naturally, designing such a system is a challenge to say the least. You can't just dump all the user's journals in the LLM history, that's expensive (input tokens ðŸ¤‘ðŸ¤‘) and not efficient. In this blog, we'll discuss both the high-level architecture and some nitty-gritty details of this system.

## Objectives

Before designing the system, it's always good to list some objectives that the system is expected to achieve. I wanted reverie's chat to be

- answer questions based on user's journals
- understand dated queries (eg. what was I doing in second week of august?)
- filter journals on user emotions (eg. when was I angry about my food?)
- list the sources it's generating its answers from


Keeping these objectives in mind, this was the high-level flowchart I designed (knee detection wasn't present in the initial draft, more about that later).

![chat pipeline flowchart](/reverie-chat-pipeline/flowchart.png)

## Tech Stack

Since all of reverie's core backend services were written in Java using Spring Boot, I used the same technology for chat services, thanks to the awesome [Spring AI](https://spring.io/projects/spring-ai/) library.

I need not mention that LLMs are used at some point in generating the answer. I used Gemini 2.5 Flash and Pro models since Google is very generous with their pricing.

For embedding models, I used the `bge-m3` model because it's multilingual, available on Cloudflare Workers AI, and has a decent enough rank on the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard).

I used pgvector as my vector database, since I was already using PostgreSQL.

## Embeddings

I'll take a detour and talk about embeddings first, since they are essential in the upcoming discussion on RAG and user queries.

Embeddings are essentially just vectors (array of numbers) that capture the semantic meaning of its input. The input can be either text, images or audio. These embeddings are generated by **embedding models**, a kind of deep learning models which convert their input to numbers. An interesting thing about these embedding models is that the vectors generated by similar inputs, like `car` and `automobile` will be similar. This goes beyond the traditional keyword search, which will not consider the words `car` and `automobile` similar. Embeddings thus power semantic search, which will be used to find relevant journals.

todo vector dbs
todos how vector search work

## RAG

But LLMs can't possibly know about user's journals, they aren't included in the LLM's training data. It will hallucinate and generate incorrect answers. One way of solving this is 

## Limitations

- No rerankers
- Hybrid search